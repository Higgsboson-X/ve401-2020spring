\section*{Assignment 8.4}

Recall that 
\begin{align*}
P = \frac{1}{n}\begin{gmatrix}[p]
1 & 1 & \cdots & 1 \\
\vdots & \vdots & & \vdots \\
1 & 1 & \cdots & 1
\end{gmatrix}\!\!\!\!, \qquad H = X(X^TX)^{-1}X^T
\end{align*}
where $X$ is the model specification matrix for multiple linear regression.
\begin{enumerate}
	\item Show that $PH = HP = P$. Conclude that $H - P$ is an orthogonal projection and that
	\begin{align*}
	\U{SS_R} = \langle (H-P)Y, (H-P)Y\rangle.
	\end{align*}
	\item Show that $\U{tr}\ P = 1$ and conclude $\U{tr}(H-P) = p$.
	\item Follow the steps in the lecture slides to show that if $\beta = (\beta_0, 0, \ldots, 0)$ (i.e., if $\beta_1 = \cdots = \beta_p = 0$), then $\U{SS_R}/\sigma^2$ follows a chi-squared distribution with $p$ degrees of freedom.
	\item Show that $(\bbone - H)(P-H) = (P-H)(\bbone-H) = 0$. Deduce that
	\begin{align*}
	\U{ran}(P-H)\subset \U{ker}(\bbone - H) \qquad\U{and} \qquad \U{ran}(\bbone-H)\subset \U{ker}(P-H).
	\end{align*}
	Explain why this means that the eigenvectors of $H-P$ for the eigenvalue 1 are also eigenvectors of $\bbone-H$ for the eigenvalue 0 and vice-versa. Construct a matrix $U$ which diagonalizes both $P-H$ and $\bbone-H$. Use $U$ to show that $\U{SS_R}$ and $\U{SS_E}$ are the sums of squares of independent standard normal variables. Deduce that $\U{SS_R}$ and $\U{SS_E}$ are independent.
\end{enumerate}
\textbf{\underline{Solution}.} 
\begin{enumerate}
	\item We know that
	\begin{align*}
	HP = \begin{gmatrix}[p]
	\overline{h}_{1\cdot} & \cdots & \overline{h}_{1\cdot} \\
	\vdots & & \vdots \\
	\overline{h}_{n\cdot} & \cdots & \overline{h}_{n\cdot}
	\end{gmatrix}\!\!\!\!, \qquad PH = \begin{gmatrix}[p]
	\overline{h}_{\cdot 1} & \cdots & \overline{h}_{\cdot n} \\
	\vdots & & \vdots \\
	\overline{h}_{\cdot 1} & \cdots & \overline{h}_{\cdot n}
	\end{gmatrix}\!\!\!\!,
	\end{align*}
	and because $HX = X, H^T = H$,
	\begin{align*}
	X_{ij} = \sum_{k=1}^n h_{ik}X_{kj},
	\end{align*}
	when $j = 1$, we have
	\begin{align*}
	1 = \sum_{k=1}^n h_{ik}X_{k1} = \sum_{k=1}^n h_{ik}\qquad\U{for\ all\ } i = 1, \ldots, n.
	\end{align*}
	Therefore,
	\begin{align*}
	HP = PH = P.
	\end{align*}
	\item We know that
	\begin{align*}
	\U{tr}\ P = \frac{1}{n} \sum_{i=1}^n 1 = 1,
	\end{align*}
	and thus
	\begin{align*}
	\U{tr}(H-P) = \U{tr}\ H - \U{tr}\ P = \U{tr}(X(X^TX)^{-1}X^T) - 1 = p + 1 - 1 = p.
	\end{align*}
	\item Since $H - P$ is an orthogonal projection, the sum of its eigenvalues is equal to the number of eigenvalues that equal 1. Since $H - P$ is symmetric, there exists $U$ consisting of columns of eigenvectors of $H-P$ such that
	\begin{align*}
	U^{-1}(H-P)U = U^T(H - P)U = D_{p} = \begin{gmatrix}[p]
	\bbone_p & 0 \\
	0 & 0
	\end{gmatrix}
	\end{align*}
	and thus $H - P = UD_pU^T$. Since given that $\beta = (\beta_0, 0, \ldots, 0)$,
	\begin{align*}
	(H-P)(X\beta + E) & = HX\beta - PX\beta + (H-P)E \\
	& = (\bbone - P)X\beta + (H-P)E \\
	& = (\bbone - P)\begin{gmatrix}[p]
	\beta_0  \\
	\vdots  \\
	\beta_0 
	\end{gmatrix}\!\!\!\! + (H-P)E = (H-P)E.
	\end{align*}
	we then have
	\begin{align*}
	\frac{\U{SS_R}}{\sigma^2} & = \frac{1}{\sigma^2}\langle (H-P)(X\beta + E), (H-P)(X\beta + E)\rangle \\
	& =  \left\langle (H-P)\frac{E}{\sigma}, (H-P)\frac{E}{\sigma}\right\rangle \\
	& = \langle Z, (H-P)Z\rangle \\
	& = \langle Z, UD_pU^T Z\rangle  \\
	& = \sum_{i=1}^{p} (U^TZ)_i^2,
	\end{align*}
	where $Z$ is standard normally distributed. Therefore, $\U{SS_R}/\sigma^2$ follows a chi-squared distribution with $p$ degrees of freedom.
	\item Since $HP = PH = P$, we have
	\begin{align*}
	(\bbone - H)(P - H) & = P - H - HP + H^2 = P - H - P + H = 0, \\
	(P - H)(\bbone - H) & = P - PH - H + H^2 = P - P - H + H = 0.
	\end{align*}
	Then for any $v\in \U{ran}(P - H)$, there exists a $u\in \R^n$ such that $v = (P-H)u$, and thus
	\begin{align*}
	(\bbone - H)(P - H)u = (\bbone - H)v = 0\quad\Rightarrow\quad v\in \U{ker}(\bbone - H).
	\end{align*}
	Similarly, for any $v\in \U{ran}(\bbone - H)$, there exists a $u\in \R^n$ such that $v = (\bbone - H)u$, and thus
	\begin{align*}
	(P - H)(\bbone - H)u = (P - H)v = 0\quad\Rightarrow\quad v\in \U{ker}(P - H).
	\end{align*}
	Therefore, $\U{ran}(P - H)\subset\U{ker}(\bbone - H)$ and $\U{ran}(\bbone - H)\subset \U{ker}(P - H)$. Then if $v$ is an eigenvector of $H-P$ for 1, then $(H-P)v = v$, which means $v\in \U{ran}(P-H)$ and thus $v\in \U{ker}(\bbone-H)$, indicating that $v$ is also an eigenvector of $\bbone-H$ for 0. It is similar with the eigenvectors of $\bbone-H$ for 1. We can construct a matrix $U$ 
	\begin{align*}
	U = (b_1, \ldots, b_p, b_{p+1}, \ldots, b_n),
	\end{align*}
	where $(b_1, \ldots, b_p)$ is an orthonormal basis of eigenvectors of $H-P$ for eigenvalue 1, and $(b_{p+1}, \ldots, b_n)$ is an orthonormal basis of eigenvectors of $H-P$ for eigenvalue 0, among which $(b_{p+2}, \ldots, b_{n})$ consists of eigenvectors of $\bbone-H$ for 1. Then it satisfies that
	\begin{align*}
	U^T(H-P)U = \begin{gmatrix}[p]
	\bbone_p & 0 \\
	0 & 0
	\end{gmatrix}\!\!\!\! =: D_{p}, \qquad U^T(\bbone-H)U = \begin{gmatrix}[p]
	0 & 0 \\
	0 & \bbone_{n-p-1}
	\end{gmatrix}\!\!\!\! =: D_{n-p},
	\end{align*}
	and $U$ diagonalizes both $P-H$ and $\bbone-H$. Since
	\begin{align*}
	\U{SS_E} & = \langle Y, (\bbone-H)Y\rangle = \langle Y, UD_{n-p-1}U^T\rangle = \sum_{i=p+2}^n (U^TY)_i^2, \\
	\U{SS_R} & = \langle Y, (H-P)Y\rangle = \langle Y, UD_pU^TY\rangle = \sum_{i=1}^p (U^TY)_i^2,
	\end{align*}
	which are independent of each other.
\end{enumerate}