\section*{Assignment 3.4}

A mathematics textbook has 200 pages on which typographical errors in the equations could occur. Suppose there are in fact five errors randomly dispersed among these 200 pages.
\begin{enumerate}
	\item What is the probability that a random sample of 50 pages will contain at least one error?
	\item How large must the random sample be to assure that at least three errors will be found with 90\% probability? (You may use a normal approximation to the binomial distribution.)
\end{enumerate}
\textbf{\underline{Solution}.}
\begin{enumerate}
	\item The problem is to randomly place the five errors in 200 pages, and each error has the same probability of being placed among the sampled pages.
	\begin{align*}
	P[\U{at\ least\ 1\ error\ in\ 50\ pages}] & = 1 - P[\U{0\ error\ in\ 50\ pages}] \\
	& = 1 - \left(\frac{200-50}{200} \right)^5 \\
	& = 76.27\%.
	\end{align*}
	\item Let the sample size be $k$. The number of selected errors follows a binomial distribution with
	\begin{align*}
	p = \frac{k}{200}, \qquad n = 5,
	\end{align*}
	and thus the mean and standard deviation are given by
	\begin{align*}
	\mu = 5p = \frac{k}{40}, \qquad \sigma = \sqrt{5p(1-p)} = \sqrt{\frac{k}{40}\left(1 - \frac{k}{200} \right)}.
	\end{align*}
	Let $X$ be the number of errors in the sample. Then
	\begin{align*}
	P[X\geq 3] \geq 90\% \quad\Rightarrow\quad P[Y + 0.5 \geq 3] = P[Y\geq 2.5] \geq 90\%,
	\end{align*}
	where $Y$ follows normal distribution. Transforming to standard normal variable $Z$, we have
	\begin{align*}
	P\left[Z\geq \frac{2.5-\mu}{\sigma} \right]\geq 0.9 \quad\Rightarrow\quad F\left[\frac{2.5-\mu}{\sigma} \right] \leq 0.1 \quad\Rightarrow \quad \frac{2.5-\mu}{\sigma} \leq -1.28,
	\end{align*}
	which gives $k\geq 150$.\\
	\textbf{\underline{Note}.} Some of you may have noticed that the requirements for ``good approximation'' specified in lecture slides are not satisfied. However, if we calculate using $p = 0.75$ and $n = 5$ for binomial distribution,
	\begin{align*}
	P[X\geq 3] = \mathtt{1 - CDF[BinomialDistribution[5, 0.75], 2]} = 0.896484,
	\end{align*}
	which is quite close to 90\%. This posterior validation shows the approximation is reasonable.
\end{enumerate}


\section*{Assignment 3.10}

Let $X = (X_1, X_2)$ be a random vector. Then we define the expectation vector and the variance-covariance matrix as follows.
\begin{align*}
\U{E}[X] := \begin{pmatrix}
\U{E}[X_1] \\ \U{E}[X_2]
\end{pmatrix}, \qquad \U{Var}\ X := \begin{pmatrix}
\U{Var}[X_1] & \U{Cov}(X_1, X_2) \\
\U{Cov}(X_2, X_1) & \U{Var}\ X_2
\end{pmatrix}.
\end{align*}
Let $A$ be a constant $2\times 2$ matrix and $Y = (Y_1, Y_2) = AX$.
\begin{enumerate}
	\item Show that $\U{E}[AX] = A\U{E}[X]$.
	\item Show that $\U{Var}(AX) = A(\U{Var}\ X)A^T$.
	\item Suppose $X_1$ and $X_2$ follow independent normal distributions with mean $\mu_1$ and $\mu_2$ and variances $\sigma_1^2$ and $\sigma_2^2$, respectively. Show that the joint density is given by
	\begin{align*}
	f_X(x) = f_X(x_1, x_2) = \frac{1}{2\pi \sqrt{\det\Sigma_X}} e^{-\frac{1}{2}\langle x - \mu_X, \Sigma_X^{-1}(x - \mu_X) \rangle}
	\end{align*}
	where $\mu_X = (\mu_1, \mu_2)$ and $\Sigma_X = \U{diag}(\sigma_1^2, \sigma_2^2)$ is the $2\times 2$ matrix with the variances on the diagonal and all other entries vanishing.
	\item  Suppose that $X_1$ and $X_2$ follow independent normal distributions with means $\mu_1, \mu_2\in \R$ and variances $\sigma_1^2, \sigma_2^2 > 0$, respectively. Let $Y = AX$ where $A$ is an invertible $n\times n$ matrix. Show that
	\begin{align}\label{eq:1}
	f_Y(y) = \frac{1}{2\pi\sqrt{|\det \Sigma_Y}|} e^{-\frac{1}{2}\langle y - \mu_Y, \Sigma_Y^{-1}(y-\mu_Y)\rangle}
	\end{align}
	where $\mu_Y = \U{E}[Y]$, $\Sigma_Y = \U{Var}\ Y$ and $\langle \cdot, \cdot\rangle$ denotes the euclidean scalar product in $\R^2$.
	\item Show that Eq. (\ref{eq:1}) can be written as
	\begin{align*}
	f_Y(y_1, y_2) = \frac{1}{2\pi \sigma_{Y_1}\sigma_{Y_2}\sqrt{1-\rho^2}} e^{-\frac{1}{2(1-\rho^2)}\left[\left(\frac{y_1 - \mu_{Y_1}}{\sigma_{Y_1}} \right)^2 - 2\rho\left(\frac{y_1-\mu_{Y_1}}{\sigma_{Y_1}} \right)\left(\frac{y_2 - \mu_{Y_2}}{\sigma_{Y_2}} \right) + \left(\frac{y_2 - \mu_{Y_2}}{\sigma_{Y_2}} \right)^2 \right]}
	\end{align*}
	where $\mu_{Y_i}$ is the mean and $\sigma_{Y_i}^2$ the variance of $Y_i, i = 1, 2$, and $\rho$ is the correlation of $Y_1$ and $Y_2$.
\end{enumerate}
\textbf{\underline{Solution}.}
\begin{enumerate}
	\item Following properties for expectation, we have
	\begin{align*}
	\U{E}[AX] = \U{E}\left[
	\begin{pmatrix}
	a_{11}X_1 + a_{12}X_2 \\ a_{21}X_1 + a_{22}X_2
	\end{pmatrix}
	\right] = \begin{pmatrix}
	a_{11}\U{E}[X_1] + a_{12}\U{E}[X_2] \\
	a_{21}\U{E}[X_1] + a_{22}\U{E}[X_2]
	\end{pmatrix} = A\U{E}[X].
	\end{align*}
	\item By definition, we have
	\begin{align*}
	\U{Var}(AX) & = \U{Var}\begin{pmatrix}
	a_{11}X_1 + a_{12}X_2 \\
	a_{21}X_1 + a_{22}X_2
	\end{pmatrix} \\
	& = \begin{pmatrix}
	\U{Var}(a_{11}X_1 + a_{12}) & \U{Cov}(a_{11}X_1 + a_{12}X_2, a_{21}X_1 + a_{22}X_2) \\
	\U{Cov}(a_{21}X_1 + a_{22}X_2, a_{11}X_1 + a_{12}X_2) & \U{Var}(a_{21}X_1 + a_{22}X_2)
	\end{pmatrix}.
	\end{align*}
	From the properties of variance and covariance, we have
	\begin{align*}
	\U{Var}(a_{11}X_1 + a_{12}X_2) & = a_{11}^2\U{Var}X_1 + a_{12}^2\U{Var}X_2 + 2a_{11}a_{12}\U{Cov}(X_1, X_2), \\
	\U{Var}(a_{21}X_1 + a_{22}X_2) & = a_{21}^2\U{Var}X_1 + a_{22}^2\U{Var}X_2 + 2a_{21}a_{22}\U{Cov}(X_1, X_2),
	\end{align*}
	and
	\begin{align*}
	\U{Cov}(a_{11}X_1 + a_{12}X_2, a_{21}X_1 + a_{22}X_2) & = \U{Cov}(a_{21}X_1 + a_{22}X_2, a_{11}X_1 + a_{12}X_2) \\
	& = a_{11}a_{21}\U{Var}(X_1) + a_{12}a_{22}\U{Var}(X_2) + \\
	& \qquad\qquad\qquad + (a_{11}a_{22} + a_{12}a_{21})\U{Cov}(X_1, X_2).
	\end{align*}
	Therefore,
	\begin{align*}
	A(\U{Var}\ X)A^T & = \begin{pmatrix}
	a_{11} & a_{12} \\
	a_{21} & a_{22}
	\end{pmatrix}\begin{pmatrix}
	a_{11}\U{Var}X_1 + a_{12}\U{Cov}(X_1, X_2) & a_{21}\U{Var}X_1 + a_{22}\U{Cov}(X_1, X_2) \\
	a_{11}\U{Cov}(X_2, X_1) + a_{12}\U{Var}X_2 & a_{21}\U{Cov}(X_1, X_2) + a_{22}\U{Var}X_2
	\end{pmatrix} \\
	& = \U{Var}(AX).
	\end{align*}
	\item We have
	\begin{align*}
	\sqrt{\det \Sigma_X} = \sigma_1\sigma_2, \qquad \Sigma_X^{-1} = \begin{pmatrix}
	1/\sigma_1^2 & 0 \\
	0 & 1/\sigma_2^2
	\end{pmatrix}.
	\end{align*}
	and
	\begin{align*}
	\langle x - \mu_X, \Sigma_X^{-1}(x - \mu_X)\rangle = \frac{(x_1 - \mu_1)^2}{\sigma_1^2} + \frac{(x_2 - \mu_2)^2}{\sigma_2^2}.
	\end{align*}
	Since $X_1$ and $X_2$ are independent, 
	\begin{align*}
	f_X(x) & = f_X(x_1, x_2) = f_{X_1}(x_1)\cdot f_{X_2}(x_2) \\
	& = \frac{1}{2\pi \sigma_1\sigma_2} e^{-\frac{1}{2}\left(\frac{(x_1 - \mu_1)^2}{\sigma_1^2} + \frac{(x_2-\mu_2)^2}{\sigma_2^2} \right)} \\
	& = \frac{1}{2\pi \sqrt{\det \Sigma_X}} e^{-\frac{1}{2}\langle x-\mu_X, \Sigma_X^{-1}(x-\mu_X)\rangle}.
	\end{align*}
	\item Since $Y = AX$, from (1) and (2) we know that
	\begin{align*}
	\mu_Y = \U{E}[AX] = A\mu_X, \qquad \Sigma_Y = A\Sigma_X A^T \quad & \Rightarrow\quad \Sigma_Y^{-1} = (A^T)^{-1}\Sigma_X^{-1}A^{-1}, \\
	& \Rightarrow\quad \det\Sigma_Y = (\det A)^2\det\Sigma_X.
	\end{align*}
	Using transformation of variables,
	\begin{align*}
	f_Y(y) & = f_Y\circ (A^{-1}y) \cdot |\det A^{-1}| \\
	& = \frac{1}{2\pi \sqrt{\det\Sigma_X}} e^{-\frac{1}{2}\langle A^{-1}y - A^{-1}\mu_Y, \Sigma_X^{-1}(A^{-1}y - A^{-1}\mu_Y)\rangle} \cdot \frac{1}{|\det A|} \\
	& = \frac{1}{2\pi \sqrt{\det \Sigma_X \cdot (\det A)^2}} e^{-\frac{1}{2}\langle y - \mu_Y, \Sigma_Y^{-1}(y - \mu_Y)\rangle} \\
	& = \frac{1}{2\pi \sqrt{|\det \Sigma_Y|}} e^{-\frac{1}{2}\langle y - \mu_Y, \Sigma_Y^{-1}(y - \mu_Y)\rangle}.
	\end{align*}
	\item Rewriting $\sqrt{|\det \Sigma_Y|}$ as
	\begin{align*}
	\sqrt{|\det \Sigma_Y|} & = \sqrt{\sigma_{Y_1}^2\sigma_{Y_2}^2 - \U{Cov}^2(Y_1, Y_2)} \\
	& = \sigma_{Y_1}\sigma_{Y_2}\sqrt{1 - \left(\frac{\U{Cov}(Y_1, Y_2)}{\sigma_{Y_1}\sigma_{Y_2}} \right)^2} \\
	& = \sigma_{Y_1}\sigma_{Y_2}\sqrt{1-\rho^2},
	\end{align*}
	and
	\begin{align*}
	\Sigma_Y = \begin{pmatrix}
	\sigma_{Y_1}^2 & \rho\sigma_{Y_1}\sigma_{Y_2} \\
	\rho\sigma_{Y_1}\sigma_{Y_2} & \sigma_{Y_2}^2
	\end{pmatrix} \quad\Rightarrow\quad \Sigma_Y^{-1} = \frac{1}{1-\rho^2}\begin{pmatrix}
	\frac{1}{\sigma_{Y_1}^2} & -\frac{\rho}{\sigma_{Y_1}\sigma_{Y_2}} \\
	-\frac{\rho}{\sigma_{Y_1}\sigma_{Y_2}} & \frac{1}{\sigma_{Y_2}^2}
	\end{pmatrix},
	\end{align*}
	we have
	\begin{align*}
	\langle y-\mu_Y, \Sigma_Y(y-\mu_Y)\rangle & = \frac{1}{1-\rho^2}\langle \begin{pmatrix}
	y_1-\mu_{Y_1} \\ y_2-\mu_{Y_2}
	\end{pmatrix}, \begin{pmatrix}
	\frac{1}{\sigma_{Y_1}^2} & -\frac{\rho}{\sigma_{Y_1}\sigma_{Y_2}} \\
	-\frac{\rho}{\sigma_{Y_1}\sigma_{Y_2}} & \frac{1}{\sigma_{Y_2}^2}
	\end{pmatrix}\rangle \\
	& = \frac{1}{1-\rho^2} \left[\left(\frac{y_1-\mu_{Y_1}}{\sigma_{Y_1}} \right)^2 - 2\rho\left(\frac{y_1-\mu_{Y_1}}{\sigma_{Y_1}} \right)\left(\frac{y_2-\mu_{Y_2}}{\sigma_{Y_2}} \right) + \left(\frac{y_2-\mu_{Y_2}}{\sigma_{Y_2}} \right)^2  \right].
	\end{align*}
	Therefore, Eq. (\ref{eq:1}) can be written as
	\begin{align*}
	f_Y(y_1, y_2) = \frac{1}{2\pi \sigma_{Y_1}\sigma_{Y_2}\sqrt{1-\rho^2}} e^{-\frac{1}{2(1-\rho^2)}\left[\left(\frac{y_1 - \mu_{Y_1}}{\sigma_{Y_1}} \right)^2 - 2\rho\left(\frac{y_1-\mu_{Y_1}}{\sigma_{Y_1}} \right)\left(\frac{y_2 - \mu_{Y_2}}{\sigma_{Y_2}} \right) + \left(\frac{y_2 - \mu_{Y_2}}{\sigma_{Y_2}} \right)^2 \right]}.
	\end{align*}
\end{enumerate}


\section*{Assignment 3.11}

A system consists of two independent components connected in series. The life span (in hours) of the component follows a Weibull distribution with $\alpha = 0.006$ and $\beta = 0.5$; the second has a lifespan in hours follows the exponential distribution with $\beta = 1/25000$.
\begin{enumerate}
	\item Find the reliability of the system at 2500 hours.
	\item Find the probability that the system will fail before 2000 hours.
	\item If the two components are connected in parallel, what is the system reliability at 2500 hours?
\end{enumerate}
\textbf{\underline{Solution}.}
\begin{enumerate}
	\item The reliability function for the two components are given by
	\begin{align*}
	R_1(t) = e^{-\alpha_1 t^{\beta_1}}, \qquad R_2(t) & = 1 - \int_0^t f_{T_2}(x)\U{d}x \\
	& = 1 + e^{-\beta_2 x}\bigg|_0^t = e^{-\beta_2 t}.
	\end{align*}
	Therefore, the reliability of the system at $t = 2500$ is given by
	\begin{align*}
	R(t) = R_1(t)\cdot R_2(t) \quad \Rightarrow\quad R(2500) = 0.7408\times 0.9048 = 0.6703.
	\end{align*}
	\item The probability that the system fail before 2000h is given by
	\begin{align*}
	P[T < 2000] = 1 - R(2000) = 1 - 0.7059 = 0.2941.
	\end{align*}
	\item The reliability at $t = 2500$ for the parallel system is
	\begin{align*}
	R(t) = 1 - (1-R_1(t))(1-R_2(t)) \quad\Rightarrow\quad R(2500) = 0.9753.
	\end{align*}
\end{enumerate}



\section*{Assignment 4.2}

Let $X_1, \ldots, X_n$ be a random sample of size $n$ from a random variable with variance $\sigma^2$. We have seen that the sample variance
\begin{align*}
S_{n-1}^2 := \frac{1}{n-1} \sum_{k=1}^n (X_k - \overline{X})^2
\end{align*}
is an unbiased estimator for $\sigma^2$. It can be shown that
\begin{align}\label{eq:2}
\U{Var}(S_{n-1}^2) = \U{MSE}(S_{n-1}^2) = \frac{1}{n}\left(\U{E}[(X - \overline{X})^4] - \frac{n-3}{n-1}\sigma^4 \right) = \frac{1}{n}\left(\gamma_2 + \frac{2n}{n-1} \right)\sigma^4
\end{align}
where $\gamma_2 := \U{E}[(X - \mu)^4]/\sigma^4 - 3$ is called the \emph{excess kurtosis} of a distribution.
\begin{enumerate}
	\item Show that if $X$ follows a normal distribution with mean $\mu$ and variance $\sigma^2$,
	\begin{align*}
	\U{MSE}(S_{n-1}^2) = \frac{2}{n-1}\sigma^4.
	\end{align*}
	\item For $a > 0$ set
	\begin{align*}
	S_a^2 := \frac{n-1}{a} S_{n-1}^2.
	\end{align*}
	Find $\U{MSE}(S_a^2)$ and show that the mean square error is minimized for
	\begin{align*}
	a = n + 1 + \frac{n-1}{n}\gamma_2.
	\end{align*}
	In the case of a normal distribution with mean $\mu$ and variance $\sigma^2$, show that this reduces to $a = n + 1$.
\end{enumerate}
\textbf{\underline{Solution}.}
\begin{enumerate}
	\item We know that,
	\begin{align*}
	\U{MSE}[S_{n-1}^2] & = \U{Var}[S_{n-1}^2] + \U{bias}^2 = \U{Var}[S_{n-1}^2].
	\end{align*}
	Since $X$ follows a normal distribution,
	\begin{align*}
	\chi_{n-1}^2 = \frac{(n-1)S_{n-1}}{\sigma^2}
	\end{align*}
	follows Chi-squared distribution with $n-1$ degrees of freedom. The variance is
	\begin{align*}
	\U{Var}[\chi_{n-1}^2] = 2(n-1) \quad\Rightarrow\quad \U{Var}[S_{n-1}^2] & = \frac{\sigma^4}{(n-1)^2}\cdot 2(n-1) = \frac{2\sigma^4}{n-1}.
	\end{align*}
	\item By definition, the MSE for $S_a^2$ is given by
	\begin{align*}
	\U{MSE}[S_a^2] & = \U{E}[S_a^4 - 2\sigma^2 S_a^2 + \sigma^4] \\
	& = \U{E}[S_a^4] - 2\sigma^2 \cdot \frac{n-1}{a}\sigma^2 + \sigma^4 \\
	& = \U{E}[S_a^4] + \left(1 - \frac{2(n-1)}{a} \right)\sigma^4.
	\end{align*}
	Using Eq. (\ref{eq:2}) and property for variance, we have
	\begin{align*}
	\U{E}[S_a^4] & = \U{Var}[S_a^2] + \U{E}[S_a^2]^2 \\
	& = \frac{(n-1)^2}{a^2}\U{Var}[S_{n-1}^2] + \frac{(n-1)^2}{a^2}\sigma^4, \\
	\U{MSE}[S_a^4] & = \frac{(n-1)^2}{a^2}\cdot\frac{1}{n}\left(\gamma_2 + \frac{2n}{n-1} \right)\sigma^4 + \frac{(n-1)^2}{a^2}\sigma^4 + \left(1 - \frac{2(n-1)}{a} \right)\sigma^4 \\
	& = \left[x^2\cdot\frac{1}{n}\left(\gamma_2+\frac{2n}{n-1} \right) + x^2 - 2x + 1 \right]\sigma^4 \qquad \left(\U{let\ } x = \frac{n-1}{a}\right) \\
	& = \left[\left(\frac{\gamma_2}{n} + \frac{2}{n-1} + 1\right)x^2 - 2x + 1 \right],
	\end{align*}
	which is maximized when
	\begin{align*}
	x = \frac{1}{\dfrac{\gamma_2}{n} + \dfrac{2}{n-1} + 1} = \frac{n-1}{a}\quad \Rightarrow\quad a = n + 1 + \frac{n-1}{n}\gamma_2.
	\end{align*}
	In case of normal distribution with mean $\mu$ and variance $\sigma^2$, since
	\begin{align*}
	\gamma_2 = \U{E}\left[\frac{(X-\mu)^4}{\sigma^4} \right] - 3 = \U{E}[Z^4] - 3,
	\end{align*}
	where $Z$ follows a standard normal distribution and $\U{E}[Z^4]$ is the 4th moment of it, given by
	\begin{align*}
	\U{E}[Z^4] = \frac{\U{d}^4m_Z(t)}{\U{d}t^4}\bigg|_{t=0} = (t^4 + 6t^2 + 3) e^{\frac{1}{2}t^2}\bigg|_{t=0} = 3,
	\end{align*}
	and thus
	\begin{align*}
	a = n + 1.
	\end{align*}
\end{enumerate}
