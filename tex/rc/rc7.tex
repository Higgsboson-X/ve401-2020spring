\section{Simple Linear Regression}

\subsection{Simple Linear Regression Model}

\begin{frame}{Simple Linear Regression Model}

\justifying
\structb{Model.} With $x$ to be the parameter, the mean of the \highlightg{response} $Y|x$ is given by 
\begin{align*}
\mu_{Y|x} = \beta_0 + \beta_1 x \qquad \U{for\ some\ } \beta_0, \beta_1 \in \R,
\end{align*}
which is equivalent to
\begin{align*}
Y|x = \beta_0 + \beta_1 x + E,
\end{align*}
where $\U{E}[E] = 0$. We want to find estimators
\begin{align*}
B_0 & := \widehat{\beta_0} = \U{estimator\ for\ } \beta_0, \qquad b_0 = \U{estimate\ for\ } \beta_0, \\
B_1 & := \widehat{\beta_1} = \U{estimator\ for\ } \beta_1, \qquad b_1 = \U{estimate\ for\ } \beta_1.
\end{align*}

\end{frame}


\begin{frame}{Simple Linear Regression Model}

\justifying
\structb{Model.} Considering $X$ as a parameter, we have a random sample of size $n$ of $(x, Y|x)$.
\begin{align*}
Y_i := Y|x_i, \qquad i = 1,\ldots, n.
\end{align*}
For each measurement $y_i$, we have a \highlightg{residual} given by
\begin{align*}
y_i = b_0 + b_1x_i + e_i.
\end{align*}
\structb{Assumptions.}
\begin{itemize}
	\justifying
	\item For each value of $x$, the random variable follows a normal distribution with variance $\sigma^2$ and mean $\mu_{Y|x} = \beta_0 + \beta_1 x$.
	\item The random variables $Y|x_1$ and $Y|x_2$ are independent if $x_1\neq x_2$.
\end{itemize}


\end{frame}


\begin{frame}{Least Squares Estimation}

\justifying
\structb{LSE.} We have the \highlightg{error sum of squares}
\begin{align*}
\U{SS_E} := \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i-(b_0+b_1x_i))^2.
\end{align*}
To minimize it, we take
\begin{align*}
\frac{\partial \U{SS_E}}{\partial b_0} & = -2\sum_{i=1}^n (y_i-b_0-b_1x_i) = 0, \\
\frac{\partial \U{SS_E}}{\partial b_1} & = -2\sum_{i=1}^n (y_i-b_0-b_1x_i)x_i = 0.
\end{align*}

\end{frame}


\begin{frame}{Least Squares Estimation}

\justifying
\structb{LSE.} We have 
\begin{align*}
b_1 = \frac{S_{xy}}{S_{xx}}, \qquad b_0 = \overline{y} - b_1\overline{x},
\end{align*}
where
\footnotesize
\begin{align*}
S_{xx} & = \sum_{i=1}^n (x_i - \overline{x})^2 = \sum_{i=1}^n x_i^2 - \frac{1}{n}\left(\sum_{i=1}^n x_i\right)^2 = \sum_{i=1}^n (x_i - \overline{x})x_i, \\
S_{yy} & = \sum_{i=1}^n (y_i - \overline{y})^2 = \sum_{i=1}^n y_i^2 - \frac{1}{n}\left(\sum_{i=1}^n y_i\right)^2 = \sum_{i=1}^n (y_i - \overline{y})y_i, \\
S_{xy} & = \sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y}) = \sum_{i=1}^n (x_i - \overline{x})y_i = \sum_{i=1}^n (y_i - \overline{y})x_i \\
& = \sum_{i=1}^n x_iy_i - \frac{1}{n}\left(\sum_{i=1}^n x_i \right)\left(\sum_{i=1}^n y_i \right).
\end{align*}

\end{frame}


\subsection{Distributions of Estimators, CIs, and Hypothesis Tests}

\begin{frame}{Distribution of $B_1$}

\structb{Theorem.} The least squares estimator $B_1$ for $\beta_1$ follows a normal distribution with
\footnotesize
\begin{align*}
\U{E}[B_1] = \beta_1, \qquad \U{Var}[B_1] = \frac{\sigma^2}{\sum (x_i-\overline{x})^2}.
\end{align*}\\
\normalsize
\only<1>{
	\justifying
	\structb{Proof.} Knowing $Y|x_i = \beta_0 + \beta_1 x_i + E$ and $\U{E}[E_i] = 0$, the expectation is given by
	\footnotesize
	\begin{align*}
	\U{E}[B_1] & = \U{E}\left[\frac{1}{S_{xx}} \sum_{i=1}^n(x_i-\overline{x})(Y_i-\overline{Y}) \right] = \U{E}\left[\frac{1}{S_{xx}}\sum (x_i-\overline{x})Y_i \right] \\
	& = \frac{1}{S_{xx}}\left(\sum (x_i-\overline{x})\U{E}[\beta_0 + \beta_1 x_i + E_i] \right) \\
	& = \frac{1}{S_{xx}}\left(\beta_1\sum(x_i-\overline{x})x_i \right) \\
	& = \beta_1.
	\end{align*}
	\normalsize
}
\only<2>{
	\justifying
	\structb{Proof.} Similarly, given $\U{Var}[E_i] = \sigma^2$, the variance is given by
	\footnotesize
	\begin{align*}
	\U{Var}[B_1] & = \frac{1}{S_{xx}^2}\U{Var}\left[\sum (x_i-\overline{x})Y_i \right] \\ 
	& = \frac{1}{S_{xx}^2} \sum (x_i-\overline{x})^2\U{Var}[\beta_0 + \beta_1 x_i + E_i] \\
	& = \frac{\sigma^2}{\sum (x_i-\overline{x})^2} \\
	& = \frac{\sigma^2}{S_{xx}}.
	\end{align*}
	\normalsize
}

\end{frame}


\begin{frame}{Distribution of $B_0$}

\structb{Theorem.} The least squares estimator $B_0$ for $\beta_0$ follows a normal distribution with
\footnotesize
\begin{align*}
\U{E}[B_0] = \beta_0, \qquad \U{Var}[B_0] = \frac{\sigma^2\sum x_i^2}{n\sum (x_i-\overline{x})^2}.
\end{align*}\\
\normalsize
\only<1>{
	\justifying
	\structb{Proof.} Using $\sum (x_i-\overline{x}) = 0$, the expectation is given by
	\footnotesize
	\begin{align*}
	\U{E}[B_0] & = \U{E}\left[\overline{Y} - \frac{\overline{x}}{S_{xx}}\sum (x_i-\overline{x})Y_i \right] \\
	& = \beta_0 + \beta_1 \overline{x} - \frac{\overline{x}}{S_{xx}}\sum (x_i-\overline{x})(\beta_0 + \beta_1 x_i) \\
	& = \beta_0 + \beta_1 \overline{x} - \frac{\overline{x}}{S_{xx}} \sum (x_i-\overline{x})x_i\beta_1 \\
	& = \beta_0.
	\end{align*}
	\normalsize
}
\only<2>{
	\justifying
	\structb{Proof.} Similarly, using $\U{Var}[\overline{E}] = \sigma^2/n$, the variance is given by
	\footnotesize
	\begin{align*}
	\U{Var}[B_0] & = \U{Var}\left[\overline{Y} - \frac{\overline{x}}{S_{xx}}\sum (x_i-\overline{x})Y_i \right] \\
	& = \U{Var}[\beta_0 + \beta_1 \overline{x} + \overline{E}] + \frac{\overline{x}^2}{S_{xx}^2} \sum(x_i-\overline{x})^2 \U{Var}[\beta_0 + \beta_1 x_i + E_i] \\
	& = \frac{\sigma^2}{n} + \frac{\overline{x}^2}{S_{xx}}\sigma^2 \\
	& = \frac{S_{xx} + \overline{x}^2}{nS_{xx}}\sigma^2 \\
	& = \frac{\sum x_i^2}{nS_{xx}}\sigma^2.
	\end{align*}
	\normalsize
}

\end{frame}

\begin{frame}{Least}
content...
\end{frame}



\subsection{Predictions and Model Analysis}


\subsection{Simple Linear Regression using Software}



\section{Multiple Linear Regression}

