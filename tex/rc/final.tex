\section{Simple Linear Regression}

\subsection{Simple Linear Regression Model}

\begin{frame}{Simple Linear Regression Model}

\justifying
\structb{Model.} We assume that
\begin{align*}
Y|x = \beta_0 + \beta_1 x + E,
\end{align*}
where $\U{E}[E] = 0$. We want to find estimators
\begin{align*}
B_0 & := \widehat{\beta_0} = \U{estimator\ for\ } \beta_0, \qquad b_0 = \U{estimate\ for\ } \beta_0, \\
B_1 & := \widehat{\beta_1} = \U{estimator\ for\ } \beta_1, \qquad b_1 = \U{estimate\ for\ } \beta_1.
\end{align*}
\structb{Assumptions.}
\begin{itemize}
	\justifying
	\item For each value of $x$, the random variable follows a normal distribution with variance $\sigma^2$ and mean $\mu_{Y|x} = \beta_0 + \beta_1 x$.
	\item The random variables $Y|x_1$ and $Y|x_2$ are independent if $x_1\neq x_2$.
\end{itemize}

\end{frame}


\begin{frame}{Least Squares Estimation}

\justifying
\structb{Least squares estimation.} We have the \highlightg{error sum of squares}
\begin{align*}
\U{SS_E} := \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i-(b_0+b_1x_i))^2.
\end{align*}
To minimize it, we take
\begin{align*}
\frac{\partial \U{SS_E}}{\partial b_0} & = -2\sum_{i=1}^n (y_i-b_0-b_1x_i) = 0, \\
\frac{\partial \U{SS_E}}{\partial b_1} & = -2\sum_{i=1}^n (y_i-b_0-b_1x_i)x_i = 0.
\end{align*}
which gives
\begin{align*}
b_1 = \frac{S_{xy}}{S_{xx}}, \qquad b_0 = \overline{y} - b_1\overline{x},
\end{align*}

\end{frame}


\begin{frame}{Useful Properties}

\justifying
\structb{Properties.}
\footnotesize
\begin{align*}
S_{xx} & = \sum_{i=1}^n (x_i - \overline{x})^2 = \sum_{i=1}^n (x_i - \overline{x})x_i = \sum_{i=1}^n x_i^2 - \frac{1}{n}\left(\sum_{i=1}^n x_i\right)^2  = \sum_{i=1}^n x_i^2 - n\overline{x}^2, \\
S_{yy} & = \sum_{i=1}^n (y_i - \overline{y})^2 = \sum_{i=1}^n (y_i - \overline{y})y_i = \sum_{i=1}^n y_i^2 - \frac{1}{n}\left(\sum_{i=1}^n y_i\right)^2 = \sum_{i=1}^n y_i^2 - n\overline{y}^2, \\
S_{xy} & = \sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y}) = \sum_{i=1}^n (x_i - \overline{x})y_i = \sum_{i=1}^n (y_i - \overline{y})x_i = \sum_{i=1}^n x_iy_i - n\overline{x}\cdot\overline{y} \\
& = \sum_{i=1}^n x_iy_i - \frac{1}{n}\left(\sum_{i=1}^n x_i \right)\left(\sum_{i=1}^n y_i \right). \\\\
b_1 & = \frac{S_{xy}}{S_{xx}}, \qquad b_0 = \overline{y} - b_1\overline{x},\qquad \U{SS_E} = S_{yy} - b_1 S_{xy}.
\end{align*}

\end{frame}


\begin{frame}{Useful Properties}

\justifying
\structb{Properties.} The last property follows from
\begin{align*}
\U{SS_E} & = \sum_{i=1}^n (y_i - \overline{y} + \overline{y} - (b_0 + b_1x_i))^2 \\
& = \sum_{i=1}^n (y_i - \overline{y} + \overline{y} - (\overline{y} - b_1\overline{x} + b_1x_i))^2 \\
& = \sum_{i=1}^n (y_i - \overline{y})^2 + \sum_{i=1}^n b_1^2(\overline{x} - x_i)^2 - 2\sum_{i=1}^n b_1(y_i-\overline{y})(x_i-\overline{x}) \\
& = S_{yy} + b_1 \cdot \frac{S_{xy}}{S_{xx}}\cdot S_{xx} - 2b_1S_{xy} \\
& = S_{yy} - b_1 S_{xy}.
\end{align*}

\end{frame}


\subsection{Estimators and Predictors}

\begin{frame}{Distribution of Estimator for Variance}

\structb{LSE for variance.} An unbiased estimator for variance $\sigma^2$ is given by
\begin{align*}
S^2 = \frac{\U{SS_E}}{n-2} = \frac{1}{n-2}\sum_{i=1}^n (Y_i-\widehat{\mu}_{Y|x_i})^2.
\end{align*}
\structb{Distribution of estimator for variance.} The statistic
\begin{align*}
\chi_{n-2}^2 = \frac{(n-2)S^2}{\sigma^2} = \frac{\U{SS_E}}{\sigma^2}
\end{align*}
follows a chi-squared distribution with $n-2$ degrees of freedom.

\end{frame}

\begin{frame}{Distribution of $B_1$}

\structb{Theorem.} The least squares estimator $B_1$ for $\beta_1$ follows a normal distribution with
\footnotesize
\begin{align*}
\U{E}[B_1] = \beta_1, \qquad \U{Var}[B_1] = \frac{\sigma^2}{\sum (x_i-\overline{x})^2} = \frac{\sigma^2}{S_{xx}}.
\end{align*}\\
\normalsize
\structb{Proof.}
\footnotesize
\begin{align*}
B_1 & = \frac{1}{S_{xx}} \sum_{i=1}^n(x_i-\overline{x})(Y_i-\overline{Y}) = \frac{1}{S_{xx}}\sum (x_i-\overline{x})Y_i \\
& = \frac{1}{S_{xx}} \sum (x_i-\overline{x})(\beta_0 + \beta_1 x_i + E_i) \\
& = \frac{1}{S_{xx}}\cdot \beta_1 S_{xx} + \frac{1}{S_{xx}} \sum (x_i-\overline{x})E_i \\
& = \beta_1 + \frac{\sum (x_i-\overline{x}) E_i}{S_{xx}}.
\end{align*}
\normalsize

\end{frame}

\begin{frame}{Distribution of $B_1$ with Estimated Variance}

\justifying
\structb{Distribution.} The statistics
\begin{align*}
T_{n-2} = \frac{B_1-\beta_1}{S/\sqrt{S_{xx}}}
\end{align*}
follows $T$-distributions with $n-2$ degrees of freedom. \\
~\\
\structb{Confidence interval.} The $100(1-\alpha)\%$ confidence interval of $\beta_1$ is given by
\begin{align*}
B_1 \pm t_{\alpha/2,n-2}\frac{S}{\sqrt{S_{xx}}}.
\end{align*}

\end{frame}

\begin{frame}{Test for Significance}

\justifying
\structb{Test for significance of regression.} Let $(x_i, Y|x_i), i = 1, \ldots, n$ be a random sample from $Y|x$. We reject
\begin{align*}
H_0: \beta_1 = 0
\end{align*}
at significance level $\alpha$ if the test statistic
\begin{align*}
T_{n-2} = \frac{B_1}{S/\sqrt{S_{xx}}}
\end{align*}
satisfies $|T_{n-2}| > t_{\alpha/2,n-2}$.

\end{frame}

\begin{frame}{Distribution of $B_0$}

\structb{Theorem.} The least squares estimator $B_0$ for $\beta_0$ follows a normal distribution with
\footnotesize
\begin{align*}
\U{E}[B_0] = \beta_0, \qquad \U{Var}[B_0] = \frac{\sigma^2\sum x_i^2}{n\sum (x_i-\overline{x})^2}.
\end{align*}\\
\normalsize
\structb{Proof.} Since
\footnotesize
\begin{align*}
B_0 & = \overline{Y} - B_1\overline{x} = \beta_0 + \beta_1\overline{x} + \overline{E} - \left(\beta_1 + \frac{\sum (x_i-\overline{x})E_i}{S_{xx}} \right) \overline{x} \\
& = \beta_0 + \overline{E} - \frac{\overline{x}\sum (x_i-\overline{x})E_i}{S_{xx}},
\end{align*}
\normalsize
we can see that
\footnotesize
\begin{align*}
\U{E}[B_0] = \beta_0, \qquad \U{Var}[B_0] & = \frac{\sigma^2}{n} + \overline{x}^2 \frac{\sigma^2}{S_{xx}} - 2\sum \U{Cov}\left[\frac{E_i}{n}, \frac{\overline{x} \sum (x_i-\overline{x})E_i}{S_{xx}} \right] \\
& = \sigma^2\cdot \frac{S_{xx} + n\overline{x}^2}{nS_{xx}} = \frac{\sigma^2 \sum x_i^2}{nS_{xx}}.
\end{align*}
\normalsize

\end{frame}

\begin{frame}{Distribution of $B_0$ with Estimated Variance}

\justifying
\structb{Distribution.} The statistics
\begin{align*}
\qquad T_{n-2} = \frac{B_0-\beta_0}{S\sqrt{\sum x_i^2}/\sqrt{nS_{xx}}}
\end{align*}
follows $T$-distributions with $n-2$ degrees of freedom. \\
~\\
\structb{Confidence interval.} The $100(1-\alpha)\%$ confidence interval of $\beta_0$ is given by
\begin{align*}
B_0 \pm t_{\alpha/2,n-2}\frac{S\sqrt{\sum x_i^2}}{\sqrt{nS_{xx}}}.
\end{align*}

\end{frame}


\begin{frame}{Distribution of Estimated Mean}

\justifying
\structb{Distribution.} The estimated mean $\widehat{\mu}_{Y|x}$ follows a normal distribution with mean and variance
\begin{align*}
\U{E}[\widehat{\mu}_{Y|x}] = \mu_{Y|x}, \qquad \U{Var}[\widehat{\mu}_{Y|x}] = \left(\frac{1}{n} + \frac{(x-\overline{x})^2}{S_{xx}}\right)\sigma^2.
\end{align*}
Therefore, the statistic
\begin{align*}
T_{n-2} = \frac{\widehat{\mu}_{Y|x} - \mu_{Y|x}}{S\sqrt{\frac{1}{n}+\frac{(x-\overline{x})^2}{S_{xx}}}}
\end{align*}
follows a $T$-distribution with $n-2$ degrees of freedom. A $100(1-\alpha)\%$ confidence interval for $\mu_{Y|x}$ is given by
\begin{align*}
\widehat{\mu}_{Y_x} \pm t_{\alpha/2,n-2}S\sqrt{\frac{1}{n}+\frac{(x-\overline{x})^2}{S_{xx}}}.
\end{align*}

\end{frame}

\begin{frame}{Distribution and CI for Predictor}

\justifying
\structb{Predictor.} The statistic $Y|x - \widehat{Y|x}$ follows a normal distribution with mean and variance
\begin{align*}
\U{E}[Y|x - \widehat{Y|x}] = 0, \qquad\U{Var}[Y|x - \widehat{Y|x}] = \left(1 +  \frac{1}{n} + \frac{(x-\overline{x})^2}{S_{xx}} \right)\sigma^2.
\end{align*}
Therefore, the statistic
\begin{align*}
T_{n-2} = \frac{Y|x - \widehat{Y|x}}{S\sqrt{1+\frac{1}{n}+\frac{(x-\overline{x})^2}{S_{xx}}}}
\end{align*}
follows a $T$-distribution with $n-2$ degrees of freedom. A $100(1-\alpha)\%$ confidence interval for $Y|x$ is given by
\begin{align*}
\widehat{Y|x} \pm t_{\alpha/2,n-2} S\sqrt{1+\frac{1}{n}+\frac{(x-\overline{x})^2}{S_{xx}} }.
\end{align*}

\end{frame}


\subsection{Model Analysis}

\begin{frame}{Model Analysis}

\structb{Crucial quantities.}
\begin{itemize}
	\justifying
	\item \highlightg{Total sum of squares}:
	\begin{align*}
	\U{SS_T} = S_{yy} = \sum_{i=1}^n (Y_i-\overline{Y})^2.
	\end{align*}
	\item \highlightg{Error sum of squared}:
	\begin{align*}
	\U{SS_E} = \sum_{i=1}^n \left(Y_i-(B_0+B_1x_i)\right)^2 = S_{yy} - B_1 S_{xy} = S_{yy} - \frac{S_{xy}^2}{S_{xx}}.
	\end{align*}
	\item \highlightg{Coefficient of determination}: the proportion of the total variation in $Y$ that is explained by the linear model.
	\begin{align*}
	R^2 = \frac{\U{SS_T} - \U{SS_E}}{\U{SS_T}} = \frac{S_{xy}^2}{S_{xx}S_{yy}}.
	\end{align*}
\end{itemize}

\end{frame}


\begin{frame}{Test for Significance with $R^2$}

\justifying
\structb{Test for significance of regression.} Let $(x_i, Y|x_i), i = 1, \ldots, n$ be a random sample from $Y|x$. We reject
\begin{align*}
H_0: \beta_1 = 0
\end{align*}
at significance level $\alpha$ if the test statistic
\begin{align*}
T_{n-2} = \frac{B_1}{S/\sqrt{S_{xx}}} = \frac{R\sqrt{n-2}}{\sqrt{1-R^2}}
\end{align*}
satisfies $|T_{n-2}| > t_{\alpha/2,n-2}$.

\end{frame}


\begin{frame}{Test for Correlation with $R^2$}

\justifying
\structb{Test for correlation.} Let $(X, Y)$ follow a bivariate normal distribution with correlation coefficient $\rho\in (-1, 1)$. Let $R$ be the estimator for $\rho$. Then we reject
\begin{align*}
H_0: \rho = 0
\end{align*}
at significance level $\alpha$ if the test statistic
\begin{align*}
T_{n-2} = \frac{R\sqrt{n-2}}{\sqrt{1-R^2}}
\end{align*}
satisfies $|T_{n-2}| > t_{\alpha/2,n-2}$.

\end{frame}


\begin{frame}{Lack-of-Fit and Pure Error}

\justifying
\structb{Source of $\U{SS_E}$.} $\U{SS_E}$ is the variance of $Y$ explained by the model.
\begin{itemize}
	\justifying
	\item \highlightg{Error sum of squares due to pure error}:
	\footnotesize
	\begin{align*}
	\U{SS_{E,pe}} := \sum_{i=1}^k \sum_{j=1}^{n_i} (Y_{ij}-\overline{Y}_i)^2 = \sum_{i=1}^k\sum_{j=1}^{n_i} Y_{ij}^2 - \sum_{i=1}^k \frac{1}{n_i}\left(\sum_{j=1}^{n_i} Y_{ij} \right)^2.
	\end{align*}
	\normalsize
	The statistic $\U{SS_{E,pe}}/\sigma^2$ follows a chi-squared distribution with $\sum_{i=1}^k (n_i - 1) = n-k$ degrees of freedom. (Recall that with sample variance $S^2$ of some sample $X_1, \ldots, X_n$, the statistic $(n-1)S^2/\sigma^2$ follows chi-squared distribution with $n-1$ d.o.f.)
	\item \highlightg{Error sum of squares due to lack of fit}:
	\footnotesize
	\begin{align*}
	\U{SS_{E,lf}} := \U{SS_E} - \U{SS_{E,pe}}.
	\end{align*}
	\normalsize
	The statistic $\U{SS_{E,lf}}/\sigma^2$ follows a chi-squared distribution with $k-2$ degrees of freedom.
\end{itemize}


\end{frame}


\begin{frame}{Testing for Lack of Fit}

\justifying
\structb{Test for lack of fit.} Let $x_1, \ldots, x_k$ be regressors and $Y_{i1}, \ldots, Y_{in_i}$, $i = 1, \ldots, k$ the measured responses at each of the regressors. Let $\U{SS_{E,pe}}$ and $\U{SS_{E,lf}}$ be the pure error and lack-of-fit sums of squares for a linear regression model. Then we reject at significance level $\alpha$
\begin{align*}
H_0: \U{the\ linear\ regression\ model\ is\ appropriate}
\end{align*}
if the test statistic
\begin{align*}
F_{k-2,n-k} = \frac{\U{SS_{E,lf}}/(k-2)}{\U{SS_{E,pe}}/(n-k)}
\end{align*}
satisfies $F_{k-2,n-k} > f_{\alpha,k-2,n-k}$.

\end{frame}



\subsection{Calculations for Simple Linear Regression}

\begin{frame}{Calculations for Simple Linear Regression}


\begin{enumerate}
	\justifying
	\item Find $\sum x_i, \sum y_i, \sum x_i^2, \sum y_i^2, \sum x_iy_i$ and calculate
	\begin{align*}
	S_{xx} & = \sum x_i^2 - \frac{1}{n}\left(\sum x_i \right)^2, \quad S_{yy} = \sum y_i^2 - \frac{1}{n}\left(\sum y_i \right)^2,\\
	S_{xy} & = \sum x_iy_i - \frac{1}{n}\left(\sum x_i \right)\left(\sum y_i \right).
	\end{align*}
	\item Obtain $b_1$ and $b_0$ by
	\begin{align*}
	b_1 = \frac{S_{xy}}{S_{xx}}, \qquad b_0 = \overline{y} - b_1\overline{x}.
	\end{align*}
	\item Calculate other quantities as required, e.g.,
	\begin{align*}
	\U{SS_E} = S_{yy} - \frac{S_{xy}}{S_{xx}}, \qquad R^2 = \frac{S_{xy}^2}{S_{xx}S_{yy}}.
	\end{align*}
\end{enumerate}


\end{frame}


\section{Multiple Linear Regression}


\subsection{Linear Algebra Basics}


\begin{frame}{Gradient of Matrix}

\justifying
\structb{Gradient.} Suppose $a, x\in \R^n, A\in \U{Mat}(n\times n; \R)$, then we have the following properties.
\begin{itemize}
	\item $\nabla_x(a^Tx) = a$, since
	\begin{align*}
	a^Tx = \sum_{i=1}^n a_ix_i \quad\Rightarrow\quad \nabla_x (x^Tx) = 
	\begin{gmatrix}[p]
	\frac{\partial a^Tx}{x_1} \\ \vdots \\ \frac{\partial a^Tx}{x_n}
	\end{gmatrix} = a.
	\end{align*}
	\item $\nabla_x (x^TAx) = 2Ax$ if $A^T = A$, since
	\begin{align*}
	x^TAx & = \sum_{i=1}^n\sum_{j=1}^n x_ia_{ij}x_j = \sum_{i=1}^n a_{ii}x_i^2 + 2\sum_{i< j} x_ia_{ij}x_j \\
	\Rightarrow\quad \frac{\partial x^TAx}{\partial x_i} & = 2\sum_{j=1}^n a_{ij}x_j\quad\Rightarrow\quad \nabla_x (x^Tx) = 2Ax.
	\end{align*}
\end{itemize}


\end{frame}


\begin{frame}{Idempotent Matrix}

\justifying
\structb{Idempotent matrix.} A $n\times n$ matrix $P$ satisfying the property that $P^2 = P$ is called idempotent. Then
\begin{align*}
(\bbone_n - P)^2 = \bbone_n - P - P + P^2 = \bbone_n - P
\end{align*}
is also idempotent. Furthermore, its eigenvalues may only be 0 or 1, since
\begin{align*}
\lambda v = Pv = P^2v = P(\lambda v) = \lambda^2v,
\end{align*}
where $v$ is an eigenvector. This gives $\lambda^2 = \lambda$. In lecture slides, $P$ is an \highlightg{orthogonal projection} if
\begin{align*}
P^2 = P, \qquad P^T = P.
\end{align*}

\end{frame}


\begin{frame}{The Spectral Theorem of Linear Algebra}

\justifying
\structb{Spectral theorem.} Let $A \in \U{Mat}(n\times n; \R)$ be a self-adjoint matrix, which means $A = A^* = A^T$. Then there exists an orthonormal basis of $\R^n$ consisting of eigenvectors of $A$. \\
~\\
\structb{Corollary of spectral theorem.} Let $A \in \U{Mat}(n\times n; \R)$ be a self-adjoint matrix. Then if $(v_1, \ldots, v_n)$ is an orthonormal basis of eigenvectors of $A$ and $U = (v_1, \ldots, v_n)$, then $U^{-1} = U^T$, and
\begin{align*}
D = U^{-1}AU = U^{T} AU
\end{align*}
is a diagonal matrix containing eigenvectors of $A$. This can be seen from
\begin{align*}
De_k = U^{-1}AUe_k = U^{-1}Av_k = U^{-1}\lambda_k v_k = \lambda_k e_k.
\end{align*}

\end{frame}

\begin{frame}{Important Results from Linear Algebra for Regression}

\structb{Results used multiple linear regression.}
\begin{itemize}
	\justifying
	\item If $A\in \U{Mat}(n\times n; \R)$ is idempotent, then $\bbone_n-A$ is idempotent, and $A$ has eigenvalues only 0 or 1.
	\item If $A\in \U{Mat}(n\times n; \R)$ is symmetric, then there exists a matrix $U = (v_1, \ldots, v_n)$ of eigenvectors of $A$ and $U^{-1} = U^T$ such that
	\begin{align*}
	D = U^{T}AU \quad\Rightarrow\quad A = UDU^T.
	\end{align*}
	\item In discussions of multiple linear regression, the two properties above hold for matrices
	\begin{align*}
	P = \frac{1}{n}\begin{gmatrix}[p]
	1 & \cdots & 1 \\
	\vdots & \ddots & \vdots \\
	1 & \cdots & 1
	\end{gmatrix} \quad \U{and}\quad H = X(X^TX)^{-1}X^T,
	\end{align*}
	and thus $\bbone_n - P$ and $\bbone_n - H$.
\end{itemize}

\end{frame}


\subsection{Multiple Linear Regression Model}

\begin{frame}{Polynomial Regression Model}

\structb{Model.} For a polynomial model, we assume that
\begin{align*}
Y|x = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_p x^p + E \quad\Leftrightarrow \quad Y = X\beta + E,
\end{align*}
where
\footnotesize
\begin{align*}
Y = \begin{gmatrix}[p]
Y_1 \\ \vdots \\ Y_n
\end{gmatrix}\!\!\!\!, \quad X = \begin{gmatrix}[p]
1 & x_1 & \cdots & x_1^p \\
\vdots & \vdots & \ddots & \vdots \\
1 & x_n & \cdots & x_n^p
\end{gmatrix}\!\!\!\!, \quad \beta = \begin{gmatrix}[p]
\beta_0 \\ \vdots \\ \beta_p
\end{gmatrix}\!\!\!\!, \quad E = \begin{gmatrix}[p]
E_1 \\ \vdots \\ E_n
\end{gmatrix}\!\!\!\!.
\end{align*}
\normalsize
\structb{Assumptions.}
\begin{itemize}
	\justifying
	\item For each value of $x$, the random variable follows a normal distribution with variance $\sigma^2$ and mean $\mu_{Y|x} = \beta_0 + \beta_1x + \cdots + \beta_p x^p$.
	\item The random variables $Y|x_1$ and $Y|x_2$ are independent if $x_1\neq x_2$.
\end{itemize}


\end{frame}


\begin{frame}{The Multilinear Model}

\structb{Model.} For a multilinear model, we assume that $Y$ depends on several factors,
\begin{align*}
Y|x = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p + E \quad\Leftrightarrow \quad Y = X\beta + E,
\end{align*}
where
\footnotesize
\begin{align*}
Y = \begin{gmatrix}[p]
Y_1 \\ \vdots \\ Y_n
\end{gmatrix}\!\!\!\!, \quad X = \begin{gmatrix}[p]
1 & x_{11} & \cdots & x_{p1} \\
\vdots & \vdots & \ddots & \vdots \\
1 & x_{1n} & \cdots & x_{pn}
\end{gmatrix}\!\!\!\!, \quad \beta = \begin{gmatrix}[p]
\beta_0 \\ \vdots \\ \beta_p
\end{gmatrix}\!\!\!\!, \quad E = \begin{gmatrix}[p]
E_1 \\ \vdots \\ E_n
\end{gmatrix}\!\!\!\!.
\end{align*}
\normalsize
\structb{Assumptions.}
\begin{itemize}
	\justifying
	\item For each value of $x$, the random variable follows a normal distribution with variance $\sigma^2$ and mean $\mu_{Y|x} = \beta_0 + \beta_1x_1 + \cdots + \beta_p x_p$.
	\item The random variables $Y|x_1$ and $Y|x_2$ are independent if $x_1\neq x_2$.
\end{itemize}

\end{frame}


\begin{frame}{Least Squares Estimation}

\structb{Least squares estimation.} For both cases, we have the error sum of squares
\begin{align*}
\U{SS_E} = \langle Y-Xb, Y-Xb\rangle = (Y-Xb)^T(Y-Xb).
\end{align*}
To minimize it, we take
\begin{align*}
\nabla_b \U{SS_E} & = \nabla_b(Y-Xb)^T(Y-Xb) \\
& = \nabla_b \left(Y^TY - Y^TXb - b^TX^TY + b^TX^TXb \right) \\
& = -2X^TY + 2X^TXb = 0 \quad\Rightarrow\quad b = (X^TX)^{-1}X^TY,
\end{align*}
where we have used since both $Y^TXb$ and $b^TX^TY$ are scalars,
\begin{align*}
b^TX^TY = (b^TX^TY)^T = Y^TXb.
\end{align*}

\end{frame}



\subsection{Model Analysis}

\begin{frame}{Error Analysis}

\structb{Crucial quantities.}
\begin{itemize}
	\justifying
	\item \highlightg{Total variation}: given orthogonal projection $P$,
	\footnotesize
	\begin{align*}
	P & := \frac{1}{n}\begin{gmatrix}[p]
	1 & \cdots & 1 \\
	\vdots & \ddots & \vdots \\
	1 & \cdots & 1
	\end{gmatrix} \quad\Rightarrow\quad (\bbone_n - P)^2 = \bbone_n - P,
	\end{align*}
	\normalsize
	giving
	\footnotesize
	\begin{align*}
	\U{SS_T} & = \langle (\bbone_n - P)Y, (\bbone_n - P)Y\rangle = \langle Y, (\bbone_n - P)Y\rangle.
	\end{align*}
	\normalsize
	\item \highlightg{Sum of squares error}: given orthogonal projection $H$,
	\footnotesize
	\begin{align*}
	H := X(X^TX)^{-1}X^T \quad\Rightarrow\quad \U{SS_E} & = \langle Y-Xb, Y-Xb\rangle \\
	& = \langle(\bbone_n - H)Y, (\bbone_n - H)Y\rangle \\
	& = \langle Y, (\bbone_n - H)Y\rangle = \langle E, (\bbone_n-H)E\rangle.
	\end{align*}
	\normalsize
	\item \highlightg{Coefficient of multiple determination}:
	\footnotesize
	\begin{align*}
	R^2 = \frac{\U{SS_R}}{\U{SS_T}}, \quad \U{SS_R} = \U{SS_T} - \U{SS_E} = \langle Y, (H-P)Y\rangle = \langle (H-P)Y,(H-P)Y\rangle.
	\end{align*}
\end{itemize}

\end{frame}

\begin{frame}{Distribution of $\U{SS_E}$}

\justifying
\structb{Distribution of sum of squares error.} The statistic given by the $\U{SS_E}$ and variance $\sigma^2$
\begin{align*}
\frac{\U{SS_E}}{\sigma^2} & = \left\langle \frac{E}{\sigma}, (\bbone_n - H)\frac{E}{\sigma}\right\rangle = \langle Z, (\bbone_n - H)Z\rangle \\
& = \langle Z, UD_{n-p-1}U^TZ\rangle = \langle U^TZ, D_{n-p-1}U^TZ\rangle \\
& = \sum_{i=1}^{n-p-1} (U^TZ)_i^2,\  \red{\left(U^TZ \sim \U{N}(0, \sigma^2 UU^T) = \U{N}(0, \sigma^2\bbone_n) \right)}
\end{align*}
follows a chi-squared distribution with $n-p-1$ degrees of freedom, where the matrix $U$ contains columns of eigenvectors of $(\bbone_n-H)$ such that
\begin{align*}
U^T(\bbone_n-H)U = D_{n-p-1}\quad\Rightarrow\quad \bbone_n - H = UD_{n-p-1}U^T.
\end{align*}

\end{frame}

\begin{frame}{Distribution of $\U{SS_E}$}

\begin{itemize}
	\justifying
	\item $\U{SS_E}/\sigma^2$ follows a chi-squared distribution with $n-p-1$ degrees of freedom.
	\item If $\beta = (\beta_0, 0, \ldots, 0)$, then $\U{SS_R}/\sigma^2$ follows a chi-squared distri-bution with $p$ degrees of freedom.
	\item $\U{SS_R}$ and $\U{SS_E}$ are independent random variables. (Fisher-Cochran theorem.)
	\item An unbiased estimator for $\sigma^2$ is given by
	\footnotesize
	\begin{align*}
	\widehat{\sigma}^2 = S^2 = \frac{\U{SS_E}}{n-p-1}.
	\end{align*}
	\normalsize
	\item The regression sum of squares can be expressed as
	\footnotesize
	\begin{align*}
	\U{SS_R} & = \langle Xb, Y\rangle - \frac{1}{n}\left(\sum_{i=1}^n Y_i \right)^2 \\
	& = b_0\sum_{i=1}^n Y_i + \sum_{j=1}^p b_j \sum_{i=1}^{n} x_{ji}Y_i - \frac{1}{2}\left(\sum_{i=1}^n Y_i \right)^2,
	\end{align*}
	\normalsize
	for multilinear model, and substitute $x_{ji}$ with $x_{i}^j$ for polynomial model.
\end{itemize}

\end{frame}

\begin{frame}{$F$-Test for Significance of Regression}

\justifying
\structb{$F$-test for significance of regression.} Let $x_1, \ldots, x_p$ be the predictor variables in a multilinear model for $Y$. Then we reject at significance level $\alpha$
\begin{align*}
H_0: \beta_1 = \beta_2 = \cdots = \beta_p = 0
\end{align*}
if the test statistic
\begin{align*}
F_{p,n-p-1} & = \frac{\U{SS_R}/p}{\U{SS_E}/(n-p-1)} = \frac{\U{SS_R}/p}{S^2} = \frac{n-p-1}{p}\frac{R^2}{1-R^2}
\end{align*}
satisfies $F_{p,n-p-1} > f_{\alpha,p,n-p-1}$.

\end{frame}
